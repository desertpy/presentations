{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest\n",
    "## A Python testing framework\n",
    "## http://pytest.org\n",
    "\n",
    "Austin Godber\n",
    "@godber\n",
    "\n",
    "DesertPy - 9/26/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing\n",
    "\n",
    "What, Why and Where?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Does your code do what it should?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Does it continue to do what it should after you modify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Three months later, how do you remember what it was supposed to do anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Tests can help *while* you're writing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Remember\n",
    "\n",
    "    Always code as if the guy who ends up\n",
    "    maintaining your code will be a violent\n",
    "    psychopath who knows where you live.\n",
    "    \n",
    "    Code for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I am sure the psychopath will appreciate a working test suite too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing Types\n",
    "\n",
    "* **unit testing** - test functionality of individual procedures\n",
    "* **integration testing** - test how parts work together (interfaces)\n",
    "* **system testing** - testing the whole system at a high level (black box, functional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing in Python\n",
    "\n",
    "* [unittest](https://docs.python.org/2/library/unittest.html) (built-in) - assertion based\n",
    "* [doctest](https://docs.python.org/2/library/doctest.html) (built-in) - example based, integrated with docstrings\n",
    "* [nose](https://nose.readthedocs.org/en/latest/) - assertion based tests with framework and plugins\n",
    "* [pytest](http://pytest.org/latest/) - framework that supports all of the above and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# pytest\n",
    "\n",
    "* Pytest can run **unittest**, **doctest** and **nose** style test suites\n",
    "* no-boilerplate\n",
    "* quick to get started, powerful to do more complex things\n",
    "* plugins\n",
    "* I started with **nose** and switched to **pytest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Test\n",
    "\n",
    "In its simplest form, a **pytest** test is a function with test in the name and will be found automatically if test is in in the filename (details later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_my_add.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_my_add.py\n",
    "def my_add(a,b):\n",
    "    return a + b\n",
    "\n",
    "def test_my_add():\n",
    "    assert my_add(2,3) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Running the basic test\n",
    "\n",
    "Run the following command in the directory with the file: \n",
    "\n",
    "``py.test``\n",
    "\n",
    "The output would look like this:\n",
    "\n",
    "<img src=\"pytest-basic-output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Failed Test Output\n",
    "\n",
    "<img src=\"pytest-basic-output-fail.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest Test Discovery\n",
    "\n",
    "How does `pytest` find its tests?\n",
    "\n",
    "* Test collection starts from the initial command line\n",
    "* Recurse into directories, unless they match `norecursedirs`\n",
    "* `test_*.py` or `*_test.py` files, imported by their package name.\n",
    "* Classes prefixed with `Test` (without an `__init__` method)\n",
    "* Functions or methods prefixed with `test_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Explaining the Basic Example\n",
    "\n",
    "* Basic example file was `test_my_add.py`\n",
    "* Executed by running: `py.test`\n",
    "* Had it been `my_add.py` \n",
    "* Execute by running `py.test my_add.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest and Doctest\n",
    "\n",
    "* By default, `pytest` only runs doctests in `*.txt` files\n",
    "* Add more with `--doctest-glob='*.rst'`\n",
    "* Run doctests in module docstrings with: `py.test --doctest-modules`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two tests: a doctest and assertion based unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_my_add2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_my_add2.py\n",
    "def my_add(a,b):\n",
    "    \"\"\" Sample doctest\n",
    "    >>> my_add(3,4)\n",
    "    7\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def test_my_add():\n",
    "    assert my_add(2,3) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 2.7.10, pytest-3.2.2, py-1.4.34, pluggy-0.4.0\r\n",
      "rootdir: /Users/godber/Workspace/presentations/pytest-godber, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 0 items                                                              \u001b[0m\u001b[1m\r",
      "collecting 1 item                                                               \u001b[0m\u001b[1m\r",
      "collected 1 item                                                                \u001b[0m\r\n",
      "\r\n",
      "test_my_add2.py .\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test test_my_add2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "only one test ran!?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 2.7.10, pytest-3.2.2, py-1.4.34, pluggy-0.4.0\r\n",
      "rootdir: /Users/godber/Workspace/presentations/pytest-godber, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 0 items                                                              \u001b[0m\u001b[1m\r",
      "collecting 1 item                                                               \u001b[0m\u001b[1m\r",
      "collecting 2 items                                                              \u001b[0m\u001b[1m\r",
      "collected 2 items                                                               \u001b[0m\r\n",
      "\r\n",
      "test_my_add2.py ..\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m=========================== 2 passed in 0.02 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test test_my_add2.py --doctest-modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "much better ... two run with **--doctest-modules** argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest and Python unittest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_my_add3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_my_add3.py\n",
    "import unittest\n",
    "\n",
    "def my_add(a,b):\n",
    "    return a + b\n",
    "\n",
    "class TestMyAdd(unittest.TestCase):\n",
    "    def test_add1(self):\n",
    "        assert my_add(3,4), 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 2.7.10, pytest-3.2.2, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/godber/Workspace/presentations/pytest-godber, inifile:\n",
      "collected 1 item                                                                \u001b[0m\u001b[1m\n",
      "\n",
      "test_my_add3.py .\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_my_add3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "runs as you would expect, without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reflection\n",
    "\n",
    "Just to be clear ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* use of **doctest** and **unittest** are optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* legacy perhaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* tests can be implemented as shown in the base example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* mix and match even"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Split your tests out into a separate file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Into a `test/` subdirectory even"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Like so:\n",
    "\n",
    "<img src=\"py-module-test-layout.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest Assertions\n",
    "\n",
    "* Python `assert` statement\n",
    "* Expecting exceptions: `pytest.raises()`\n",
    "* Expecting failure (coming up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Running Tests on Module Code\n",
    "\n",
    "Environment setup can differ somewhat, but if you wanted to run the code shown below you have two choices:\n",
    "\n",
    "* Install `badstats` module with `pip install -e .`\n",
    "* Run pytest in `badstats` top directory with PYTHONPATH set: `PYTHONPATH=. py.test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# New Example\n",
    "\n",
    "The following `badstats` module will be used in upcoming examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# %load badstats/badstats/__init__.py\n",
    "def _sum(data):\n",
    "    total = 0\n",
    "    for d in data:\n",
    "        total += d\n",
    "    return total\n",
    "\n",
    "def mean(data):\n",
    "    n = len(data)\n",
    "    return _sum(data) / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# %load badstats/tests/test_badstats_sum_a.py\n",
    "from badstats import _sum\n",
    "\n",
    "def test_sum_simple():\n",
    "    data = (1, 2, 3, 4)\n",
    "    assert _sum(data) == 10\n",
    "\n",
    "def test_sum_fails():\n",
    "    data = (1.2, -1.0)\n",
    "    assert _sum(data) == 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 2.7.10, pytest-3.2.2, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/godber/Workspace/presentations/pytest-godber/badstats, inifile:\n",
      "collected 2 items                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "badstats/tests/test_badstats_sum_a.py .F\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[1m\u001b[31m________________________________ test_sum_fails ________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_sum_fails():\u001b[0m\n",
      "\u001b[1m        data = (1.2, -1.0)\u001b[0m\n",
      "\u001b[1m>       assert _sum(data) == 0.2\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 0.19999999999999996 == 0.2\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 0.19999999999999996 = _sum((1.2, -1.0))\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mbadstats/tests/test_badstats_sum_a.py\u001b[0m:9: AssertionError\n",
      "\u001b[1m\u001b[31m====================== 1 failed, 1 passed in 0.03 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test badstats/tests/test_badstats_sum_a.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Floating Point is Hard :-/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let's fix it later\n",
    "* ... but keep the test as a reminder\n",
    "* ... and mark it as expected to fail with `xfail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# %load badstats/tests/test_badstats_sum_b.py\n",
    "import pytest\n",
    "from badstats import _sum\n",
    "\n",
    "def test_sum_simple():\n",
    "    data = (1, 2, 3, 4)\n",
    "    assert _sum(data) == 10\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_sum_fails():\n",
    "    data = (1.2, -1.0)\n",
    "    assert _sum(data) == 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 2.7.10, pytest-3.2.2, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/godber/Workspace/presentations/pytest-godber/badstats, inifile:\n",
      "collected 2 items                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "badstats/tests/test_badstats_sum_b.py .x\n",
      "\n",
      "\u001b[32m\u001b[1m===================== 1 passed, 1 xfailed in 0.03 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test badstats/tests/test_badstats_sum_b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest Xfail, Conditions and More\n",
    "\n",
    "* Modfying test execution using `pytest` module\n",
    "* Use of the decorator `@pytest.mark.xfail`.\n",
    "* Skip tests with conditionals and `@pytest.mark.skipif`\n",
    "* Custom Markers: `@pytest.mark.NAME`\n",
    "  * `@pytest.mark.webtest`\n",
    "  * `py.test -v -m webtest`\n",
    "* See [mark documentation](http://pytest.org/latest/mark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Extended Xfail Example\n",
    "\n",
    "* Variable Use\n",
    "* Xfail conditional (and reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# %load badstats/tests/test_badstats_sum_c.py\n",
    "import pytest\n",
    "import sys\n",
    "from badstats import _sum\n",
    "xfail = pytest.mark.xfail\n",
    "\n",
    "def test_sum_simple():\n",
    "    data = (1, 2, 3, 4)\n",
    "    assert _sum(data) == 10\n",
    "\n",
    "@xfail(sys.platform in ['darwin', 'linux'], reason='requires windows')\n",
    "def test_sum_fails():\n",
    "    data = (1.2, -1.0)\n",
    "    assert _sum(data) == 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 2.7.10, pytest-3.2.2, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/godber/Workspace/presentations/pytest-godber/badstats, inifile:\n",
      "collected 2 items                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "badstats/tests/test_badstats_sum_c.py .x\n",
      "\n",
      "\u001b[32m\u001b[1m===================== 1 passed, 1 xfailed in 0.03 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test badstats/tests/test_badstats_sum_c.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametrize\n",
    "\n",
    "When you want to test a number of inputs on the same function, use `@pytest.mark.parametrize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# %load badstats/tests/test_badstats_sum_param.py\n",
    "import pytest\n",
    "from badstats import _sum\n",
    "\n",
    "@pytest.mark.parametrize(\"input,expected\", [\n",
    "    ((1, 2, 3, 4), 10),\n",
    "    ((0, 0, 1, 5), 6),\n",
    "    pytest.mark.xfail(((1.2, -1.0), 0.2)),\n",
    "])\n",
    "def test_sum(input, expected):\n",
    "    assert _sum(input) == expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 2.7.10, pytest-3.2.2, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/godber/Workspace/presentations/pytest-godber/badstats, inifile:\n",
      "collected 3 items                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "badstats/tests/test_badstats_sum_param.py ..x\n",
      "\n",
      "\u001b[32m\u001b[1m===================== 2 passed, 1 xfailed in 0.03 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test badstats/tests/test_badstats_sum_param.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note there appear to have been three tests executed.\n",
    "\n",
    "You can inspect what tests are 'found' by using the `py.test --collect-only`.\n",
    "\n",
    "<img src='pytest-collect-only.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest Fixtures\n",
    "\n",
    "\"The purpose of test fixtures is to provide a fixed baseline upon which tests can reliably and repeatedly execute. pytest fixtures offer dramatic improvements over the classic xUnit style of setup/teardown functions.\"\n",
    "\n",
    "However, [classic xUnit style Setup/teardown functions](http://pytest.org/latest/xunit_setup.html) are still available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# %load badstats/tests/test_badstats_fixture.py\n",
    "import pytest\n",
    "import badstats\n",
    "\n",
    "@pytest.fixture\n",
    "def data():\n",
    "    return (1.0, 2.0, 3.0, 4.0)\n",
    "\n",
    "def test_sum_simple(data):\n",
    "    assert badstats._sum(data) == 10.0\n",
    "\n",
    "def test_mean_simple(data):\n",
    "    assert badstats.mean(data) == 2.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 2.7.10, pytest-3.2.2, py-1.4.34, pluggy-0.4.0\r\n",
      "rootdir: /Users/godber/Workspace/presentations/pytest-godber/badstats, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 0 items                                                              \u001b[0m\u001b[1m\r",
      "collecting 2 items                                                              \u001b[0m\u001b[1m\r",
      "collected 2 items                                                               \u001b[0m\r\n",
      "\r\n",
      "badstats/tests/test_badstats_fixture.py ..\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m=========================== 2 passed in 0.01 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test badstats/tests/test_badstats_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other Things\n",
    "\n",
    "* Stand Alone Self Encapsulated Tests: `py.test --genscript=runtests.py`\n",
    "* Stop output capture (see `print()` statements): `py.test -s`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Topics for Lightning Talks\n",
    "\n",
    "* Grouping Tests in Classes\n",
    "* [xUnit Style Setup/Teardown](http://pytest.org/latest/xunit_setup.html)\n",
    "* Understanding Fixture Details: (scope, finalize, parameterization)\n",
    "* Integration with other tools: **`tox`**, `setuptools`\n",
    "* Advanced Reporting\n",
    "* Handling Command Line Options\n",
    "* Plugins and configuration using `conftest.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You\n",
    "\n",
    "Austin Godber **@godber**\n",
    "\n",
    "http://desertpy.com/pages/presentations.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
