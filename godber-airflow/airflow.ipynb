{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Programming Apache Airflow with Python\n",
    "\n",
    "\n",
    "<img style=\"float: right;\" src=\"airflow-small.png\" />\n",
    "\n",
    "Austin Godber\n",
    "\n",
    "@godber\n",
    "\n",
    "http://desertpy.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Airflow is a platform to programmatically author, schedule and monitor workflows.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Airflow is not a data streaming solution.\"\n",
    "\n",
    "It's more about running a workflow than it is about the things the workflow acomplishes.\n",
    "\n",
    "(NOTE: All unattributed quotes come from the [official Airflow docs](http://airflow.apache.org))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Top Level Features (1/2)\n",
    "\n",
    "* Author, execute and track workflows or Directed Acyclic Graphs (DAGs)\n",
    "* Complete CLI and web based GUI\n",
    "* Many different task types, or \"Operators\" (e.g. Bash, Python)\n",
    "* Operator templating with Jinja\n",
    "* Connections expose external systems to DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Top Level Features (2/2)\n",
    "\n",
    "* Variables store arbitrary content accessible by DAGs\n",
    "* Extensible Web Interface\n",
    "* XComs allow tasks to (slowly) exchange (limited) info\n",
    "* Branching allows conditional execution of tasks\n",
    "* SubDAGs make repeating tasks easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"airflow.gif\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "Quick and dirty sequential execution, SQLITE backed dev setup:\n",
    "\n",
    "```bash\n",
    "mkvirtualenv -p `which python3` airflow\n",
    "export SLUGIFY_USES_TEXT_UNIDECODE=yes\n",
    "pip3 install apache-airflow[mysql,redis,slack,crypto,password,ssh]\n",
    "airflow initdb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Start the webserver and start the simple scheduler:\n",
    "\n",
    "```bash\n",
    "# set host to 127.0.0.1 and use SSH tunneling for access\n",
    "sed -i 's/^web_server_host.*$/web_server_host = 127.0.0.1/' airflow.cfg\n",
    "airflow webserver -p 8080\n",
    "# in another terminal\n",
    "workon airflow\n",
    "airflow scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Setup Options\n",
    "\n",
    "* There is a lengthy list of extra packages you can install\n",
    "  * Different BD Backends - MySQL, PostgreSQL\n",
    "  * Different Executors - Dask, Celery, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial\n",
    "\n",
    "The [example tutorial from the docs](http://airflow.apache.org/tutorial.html)\n",
    "\n",
    "Loading modules:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator \\\n",
    "  import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Specify `default_args` to be passed to DAG constructor:\n",
    "\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2018, 10, 1),\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "dag = DAG('tutorial', default_args=default_args)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define the tasks (Operators) to run ...\n",
    "\n",
    "```python\n",
    "t1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag)\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='sleep',\n",
    "    bash_command='sleep 5',\n",
    "    retries=3,\n",
    "    dag=dag)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define a Jinja template for a templated operator:\n",
    "\n",
    "```python\n",
    "templated_command = \"\"\"\n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7)}}\"\n",
    "        echo \"{{ params.my_param }}\"\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use the template defined above:\n",
    "\n",
    "```python\n",
    "t3 = BashOperator(\n",
    "    task_id='templated',\n",
    "    bash_command=templated_command,\n",
    "    params={'my_param': 'Parameter I passed in'},\n",
    "    dag=dag)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define the relationships between tasks:\n",
    "\n",
    "```python\n",
    "t2.set_upstream(t1)\n",
    "t3.set_upstream(t1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"One thing to wrap your head around (it may not be very intuitive for everyone at first) is that this Airflow Python script is really just a configuration file specifying the DAG’s structure as code. The actual tasks defined here will run in a different context from the context of this script. Different tasks run on different workers at different points in time, which means that this script cannot be used to cross communicate between tasks. Note that for this purpose we have a more advanced feature called XCom.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"People sometimes think of the DAG definition file as a place where they can do some actual data processing - that is not the case at all! The script’s purpose is to define a DAG object. It needs to evaluate quickly (seconds, not minutes) since the scheduler will execute it periodically to reflect the changes if any.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DagRuns\n",
    "\n",
    "Understanding `start_date` and `schedule_interval` is key to understanding how your DAGs are going to run.\n",
    "\n",
    "\"The first `DagRun` to be created will be based on the `min(start_date)` for all your task. From that point on, the scheduler creates new `DagRun`s based on your `schedule_interval` and the corresponding task instances run as your dependencies are met.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DagRuns Example\n",
    "\n",
    "So, if today is October 24th, 2018 and you create a DAG, you set your `start_date` to `2018-10-01` and use the default `schedule_interval` which is daily, the first `DagRun` will be with a date stamp (`ds`) of `2018-10-01` and will run immediately.  Then the next one will run with `ds` of `2018-10-02`.  These may run immediately depending on your dependencies.  A total of 23 `DagRun`s will run, up to the current date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Operators\n",
    "\n",
    "* An operator describes the task to be run and all operators inherit from the `BaseOperator`.\n",
    "* There are three types of operators:\n",
    "  * **actions** - tell another system to perform an action\n",
    "  * **transfer** - moves data from one system to another\n",
    "  * **sensors** - will keep running until a condition is met (e.g. file appears in S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Operators (cont.)\n",
    "\n",
    "* `BashOperator`, `PythonOperator`, `PythonVirtualenvOperator`, `SimpleHttpOperator`\n",
    "* `BranchPythonOperator`, `CheckOperator` (sql check)\n",
    "* `EmailOperator`, `SlackOperator`, `DiscordWebhookOperator`\n",
    "* `DockerOperator`, `*Hive*`, `(MySql|Postgres|Sqlite)Operator`\n",
    "* `GenericTransfer` (sql), `S3TransferOperator`\n",
    "* `ExternalTaskSensor`, `HdfsSensor`, `HttpSensor`, `S3KeySensor`\n",
    "\n",
    "There's too many, go here: https://airflow.apache.org/code.html#id3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Template Variables\n",
    "\n",
    "<img src=\"airflow-variables-pt1.png\" />\n",
    "\n",
    "https://airflow.apache.org/code.html#default-variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Variables (cont.)\n",
    "\n",
    "* Object attributes and methods can be accessed with dot notation `{{task.owner}}`\n",
    "  * See individual Model documentation for details on attributes\n",
    "* Variables set in Airflow can be accessed through `var`\n",
    "  * `{{var.value.<VARIABLE_NAME>}}`\n",
    "  * `{{var.json.<VARIABLE_NAME>}}`\n",
    "    * `{{var.json.<VARIABLE_NAME>.<KEY>}}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Template Macros\n",
    "\n",
    "Macros can be used in your templates to manipulate variables or strings\n",
    "\n",
    "* `ds_add(ds, days)` - add or subtract days from date string\n",
    "* `ds_format(ds, input_format, output_format)` - reformat date string\n",
    "* `random()` - returns random number between 0 and 1 (excluding 1)\n",
    "\n",
    "https://airflow.apache.org/code.html#macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* IDEMPOTENT!!!\n",
    "* Manage Connections and Secrets with Connections and Variables\n",
    "* `airflow delete_dag my_dag_id`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources\n",
    "\n",
    "* https://github.com/jghoman/awesome-apache-airflow\n",
    "* https://github.com/airflow-plugins/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* https://github.com/airflow-plugins/Getting-Started/blob/master/Tutorial/creating-ui-modification.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ airflow test plot-phx-gsod get_data 2018-10-15\n",
    "[2018-10-17 03:18:22,683] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2018-10-17 03:18:22,799] {models.py:258} INFO - Filling up the DagBag from /srv/airflow/airflow/dags\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ airflow list_tasks plot-phx-gsod\n",
    "[2018-10-17 03:19:36,323] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "[2018-10-17 03:19:36,428] {models.py:258} INFO - Filling up the DagBag from /srv/airflow/airflow/dags\n",
    "extract_data\n",
    "get_data\n",
    "get_isd_history\n",
    "plot_phx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
